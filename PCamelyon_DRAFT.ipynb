{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd0f2296-bf82-4efb-bd1b-1135090e8602",
   "metadata": {},
   "source": [
    "# PCamelyon Project: Prototype for MSX Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e1d294-5897-4d3d-93df-6be10b0b892c",
   "metadata": {},
   "source": [
    "## Phase 0: Setup, Familiarization, and Initial Exploration\n",
    "**Goal**: Get everything set up, load the PCam dataset, and understand its basic characteristics.\n",
    "\n",
    "**Tasks**:\n",
    "\n",
    "- **Environment Setup**:\n",
    "    - Create a dedicated Python environment (e.g., using Conda or venv).\n",
    "    - Install core libraries: PyTorch, torchvision, HuggingFace datasets, transformers (for potential ViT later), scikit-learn, Matplotlib, Pillow/OpenCV, NumPy.\n",
    "\n",
    "- **Dataset Loading & Inspection**:\n",
    "    - Use HuggingFace datasets to load PCam.\n",
    "    - Examine the dataset splits (train, validation, test).\n",
    "    - Check image dimensions (96x96 pixels), color channels (RGB).\n",
    "    - Analyze label distribution (is it balanced?).\n",
    "    - Visualize a few sample patches from each class (tumor vs. no tumor).\n",
    "\n",
    "**Learning Objectives Addressed**:\n",
    "- Understand the initial steps of an ML/DL pipeline.\n",
    "- Learn effective data loading for histology image patches.\n",
    "- Simulate working with a binary classification task (\"disease presence/absence\").\n",
    "\n",
    "**Transferability to MSX Project**:\n",
    "- **Environment Setup**: Standard first step for any project.\n",
    "- **Data Loading/Inspection**: Crucial for MSX. While PCam patches are pre-made, MSX will involve loading WSIs and then creating patches. Understanding patch characteristics here (size, content) will be informative. Visualizing samples helps build intuition for the data.\n",
    "- **Binary Task**: Directly mirrors the primary goal of MSX detection (presence/absence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871f4758-fabe-4c54-a871-8f071a5c9df9",
   "metadata": {},
   "source": [
    "### Task 1: Environment Setup\n",
    "\n",
    "- Created a dedicated Python Environment using Conda\n",
    "- Install Core Libraries"
   ]
  },
  {
   "cell_type": "code",
   "id": "ecff73e79eadf1a0",
   "metadata": {},
   "source": [
    "# Install core libraries for the project\n",
    "!pip install torch torchvision torchaudio -q\n",
    "!pip install datasets -q\n",
    "!pip install transformers -q\n",
    "!pip install scikit-learn -q\n",
    "!pip install matplotlib seaborn -q\n",
    "!pip install opencv-python pillow -q\n",
    "!pip install \"numpy<2\" -q\n",
    "!pip install tqdm -q"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bcbbedf3ee9ffbf5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:49:39.338683Z",
     "start_time": "2025-06-02T15:49:39.330378Z"
    }
   },
   "source": [
    "# Verify installation of core libraries and their versions\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import torchvision\n",
    "import torch\n",
    "import datasets\n",
    "import transformers\n",
    "import sklearn\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PIL\n",
    "import numpy as np\n",
    "print(\"Core libraries installed successfully:\")\n",
    "print(f\"torch: {torch.__version__}\")\n",
    "print(f\"torchvision: {torchvision.__version__}\")\n",
    "print(f\"datasets: {datasets.__version__}\")\n",
    "print(f\"transformers: {transformers.__version__}\")\n",
    "print(f\"scikit-learn: {sklearn.__version__}\")\n",
    "print(f\"matplotlib: {matplotlib.__version__}\")\n",
    "print(f\"seaborn: {sns.__version__}\")\n",
    "print(f\"Pillow: {PIL.__version__}\")\n",
    "print(f\"numpy: {np.__version__}\")\n",
    "\n",
    "# supress warnings from libraries\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "plt.style.use('ggplot')  # Use a nice style for plots\n",
    "\n",
    "# Set default figure size for plots\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# Extra libraries, order later\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms # Already imported\n",
    "from torch.utils.data import DataLoader # Already imported\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    ConfusionMatrixDisplay\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries installed successfully:\n",
      "torch: 2.7.0\n",
      "torchvision: 0.22.0\n",
      "datasets: 3.6.0\n",
      "transformers: 4.52.4\n",
      "scikit-learn: 1.6.1\n",
      "matplotlib: 3.10.3\n",
      "seaborn: 0.13.2\n",
      "Pillow: 11.2.1\n",
      "numpy: 1.26.4\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-02T15:49:41.444555Z",
     "start_time": "2025-06-02T15:49:41.439813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Argument Parsing ---\n",
    "parser = argparse.ArgumentParser(description=\"PatchCamelyon Training and Evaluation Script\")\n",
    "parser.add_argument(\n",
    "    '--hpc_mode',\n",
    "    type=int,\n",
    "    default=0,\n",
    "    choices=[0, 1],\n",
    "    help=\"Set to 1 for HPC console mode (saves plots, no plt.show()). Default is 0 (interactive mode, uses plt.show()).\"\n",
    ")\n",
    "parser.add_argument(\n",
    "    '--output_dir',\n",
    "    type=str,\n",
    "    default='./results_and_models',\n",
    "    help=\"Directory to save models and plots.\"\n",
    ")\n",
    "\n",
    "# Check if the script is running in an iPython/Jupyter environment\n",
    "if 'ipykernel_launcher.py' in sys.argv[0]:\n",
    "    # In Jupyter, use default values or manually set them in the notebook\n",
    "    print(\"Running in Jupyter/iPython mode. Using default arguments or manually set values.\")\n",
    "    args = parser.parse_args(args=[]) # Parse with an empty list to use defaults\n",
    "else:\n",
    "    # Running from command line, parse arguments normally\n",
    "    args = parser.parse_args()\n",
    "\n",
    "# Global variable based on the argument\n",
    "HPC_MODE = (args.hpc_mode == 1)\n",
    "OUTPUT_DIR = args.output_dir\n",
    "\n",
    "# Create the output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"HPC Mode: {'Enabled' if HPC_MODE else 'Disabled'}\")\n",
    "print(f\"Output directory for plots/models: {OUTPUT_DIR}\")"
   ],
   "id": "fc98421446164bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running in Jupyter/iPython mode. Using default arguments or manually set values.\n",
      "HPC Mode: Disabled\n",
      "Output directory for plots/models: ./results_and_models\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Custom Plotting Function\n",
    "def display_or_save_plot(figure, filename_base, hpc_mode_flag=HPC_MODE, output_path_base=OUTPUT_DIR):\n",
    "    \"\"\"\n",
    "    Displays a Matplotlib plot or saves it to a file based on hpc_mode.\n",
    "\n",
    "    Args:\n",
    "        figure (matplotlib.figure.Figure): The Matplotlib figure object to display/save.\n",
    "                                           If None, uses plt.gcf() (get current figure).\n",
    "        filename_base (str): The base name for the saved file (e.g., \"learning_curves\").\n",
    "                             Will have \".png\" appended.\n",
    "        hpc_mode_flag (bool): If True, saves the plot. If False, shows the plot.\n",
    "        output_path_base (str): The base directory where plots will be saved.\n",
    "    \"\"\"\n",
    "    if figure is None:\n",
    "        figure = plt.gcf() # Get current figure if not explicitly passed\n",
    "\n",
    "    if hpc_mode_flag:\n",
    "        # Ensure the output directory for plots exists within the base output directory\n",
    "        plot_dir = os.path.join(output_path_base, \"plots\")\n",
    "        os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "        save_path = os.path.join(plot_dir, f\"{filename_base}.png\")\n",
    "        figure.savefig(save_path, bbox_inches='tight', dpi=300)\n",
    "        print(f\"Plot saved to: {save_path}\")\n",
    "        plt.close(figure) # Close the figure to free memory, important in long-running scripts\n",
    "    else:\n",
    "        plt.show()"
   ],
   "id": "76a031bb3afb3146",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fa67c824-d388-40b5-83d3-0da0d148b231",
   "metadata": {},
   "source": [
    "### Task 2: Dataset Loading & Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931007b-181f-487c-b515-7e485d139e2f",
   "metadata": {},
   "source": [
    "Import libraries and load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "id": "dcd84e14-ee2a-4228-9144-aa482b59b835",
   "metadata": {},
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load PatchCamelyon dataset using library, \n",
    "# this will download dataset and cache it for future use if not available locally.\n",
    "\n",
    "try:\n",
    "    pcam = load_dataset(\"1aurent/PatchCamelyon\")\n",
    "    print(\"Successfully loaded PatchCamelyon dataset.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading dataset: {e}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "91c5f68b5aa04c44",
   "metadata": {},
   "source": [
    "Once dataset is loaded, we can check its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "id": "6974663b22bd1188",
   "metadata": {},
   "source": [
    "print(\"Dataset structure:\")\n",
    "print(pcam)\n",
    "\n",
    "# Calculate the total number of samples in the dataset and distribution across splits\n",
    "total_samples = sum(len(pcam[split]) for split in pcam)\n",
    "print(f\"\\nTotal samples in the dataset: {total_samples}\")\n",
    "\n",
    "# Check the available splits in the dataset\n",
    "print(\"\\nAvailable splits in the dataset:\")\n",
    "for split in pcam:\n",
    "    split_samples = len(pcam[split])\n",
    "    print(f\"  - {split}: {split_samples} samples - {split_samples / total_samples * 100:.2f}%\")\n",
    "\n",
    "# Check feature names and types\n",
    "print(\"\\nFeature names and types:\")\n",
    "for split in pcam:\n",
    "    print(f\"\\n{split} split features:\")\n",
    "    for feature_name, feature_type in pcam[split].features.items():\n",
    "        print(f\"  - {feature_name}: {feature_type}\")\n",
    "\n",
    "# Check the first few samples in the training set\n",
    "print(\"\\nFirst few samples in the training set:\")\n",
    "for i in range(3):\n",
    "    print(f\"Sample {i}:\")\n",
    "    print(pcam['train'][i])\n",
    "    print()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "36bee705560dbc87",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40b238f90405b326",
   "metadata": {},
   "source": [
    "Check image dimensions, color channels, and label distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "253fb3f066262c5",
   "metadata": {},
   "source": [
    "# get the first sample from the training set\n",
    "sample = pcam['train'][0]\n",
    "\n",
    "image = sample['image']\n",
    "label = sample['label']\n",
    "\n",
    "print(f\"\\nFirst training sample image type: {type(image)}\")\n",
    "print(f\"Image dimensions: {image.size} (width x height)\")\n",
    "print(f\"Image mode (color channels): {image.mode}\")\n",
    "print(f\"Label: {label}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4f818f3c905eea4",
   "metadata": {},
   "source": [
    "Class labels are binary (0 = False for no tumor, 1 = True for tumor)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eecb5328bd4483",
   "metadata": {},
   "source": [
    "Let's check if the dataset is balanced in terms of class distribution."
   ]
  },
  {
   "cell_type": "code",
   "id": "80e515efb76b7497",
   "metadata": {},
   "source": [
    "def count_labels(dataset):\n",
    "    '''Count the number of samples for each label in the dataset.'''\n",
    "    label_counts = {0: 0, 1: 0}\n",
    "    for sample in dataset:\n",
    "        label_counts[sample['label']] += 1\n",
    "    return label_counts"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ded4a702647b6e24",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "4fd35b5c179ea39b",
   "metadata": {},
   "source": [
    "import collections\n",
    "\n",
    "train_labels_raw = pcam['train']['label']  # This will be a list like [False, True, False, ...]\n",
    "\n",
    "# Use collections.Counter to count the occurrences of False and True.\n",
    "# This is highly optimized for counting items in a list.\n",
    "raw_counts = collections.Counter(train_labels_raw)\n",
    "\n",
    "# Map these boolean counts to integer keys (0 for False, 1 for True)\n",
    "label_counts_for_plot = {\n",
    "    0: raw_counts.get(False, 0),  # Count for 'False' (no tumor), map to key 0\n",
    "    1: raw_counts.get(True, 0)    # Count for 'True' (tumor), map to key 1\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "\n",
    "keys_for_plot = list(label_counts_for_plot.keys())\n",
    "values_for_plot = list(label_counts_for_plot.values())\n",
    "\n",
    "sns.barplot(x=keys_for_plot, y=values_for_plot, palette='viridis', hue=keys_for_plot, legend=False)\n",
    "\n",
    "plt.title('Label Distribution in Training Set')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.xticks([0, 1], ['No Tumor (0)', 'Tumor (1)'])\n",
    "\n",
    "# Show the percentage or count on top of each bar\n",
    "for i, count_val in enumerate(values_for_plot):\n",
    "    plt.text(i, count_val / 2, f\"{count_val}\", ha='center', color='white', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "10b3eb6030efba90",
   "metadata": {},
   "source": [
    "Visualize a few sample patches from each class (tumor vs. no tumor)."
   ]
  },
  {
   "cell_type": "code",
   "id": "f6a00dd3cb76c00c",
   "metadata": {},
   "source": [
    "def show_samples(dataset, num_samples_per_class=5):\n",
    "    \"\"\"\n",
    "    Efficiently displays a few random sample images from the dataset for each class.\n",
    "    Args:\n",
    "        dataset (datasets.Dataset): A HuggingFace dataset split (e.g., pcam['train']).\n",
    "        num_samples_per_class (int): Number of samples to show for each class.\n",
    "    \"\"\"\n",
    "    # Attempt to get class names from features for nice titles\n",
    "    try:\n",
    "        class_feature = dataset.features['label']\n",
    "        class_names_map = {\n",
    "            False: class_feature.names[0] if hasattr(class_feature, 'names') else \"Class False/0 (No Tumor)\",\n",
    "            True: class_feature.names[1] if hasattr(class_feature, 'names') else \"Class True/1 (Tumor)\"\n",
    "        }\n",
    "        target_labels_for_display = [False, True] # We want to display 'no_tumor' then 'tumor'\n",
    "    except (KeyError, AttributeError):\n",
    "        print(\"Warning: Could not automatically determine class names. Using raw labels.\")\n",
    "        class_names_map = {False: \"Label False\", True: \"Label True\"}\n",
    "        target_labels_for_display = [False, True]\n",
    "\n",
    "    fig, axes = plt.subplots(len(target_labels_for_display), num_samples_per_class,\n",
    "                             figsize=(num_samples_per_class * 3, len(target_labels_for_display) * 3))\n",
    "\n",
    "    # Ensure axes is always 2D for consistent indexing, even if only one class or one sample\n",
    "    if len(target_labels_for_display) == 1 and num_samples_per_class == 1:\n",
    "        axes = np.array([[axes]])\n",
    "    elif len(target_labels_for_display) == 1:\n",
    "        axes = np.array([axes])\n",
    "    elif num_samples_per_class == 1:\n",
    "        axes = axes.reshape(-1,1)\n",
    "\n",
    "\n",
    "    all_labels = dataset['label']\n",
    "\n",
    "    for i, label_to_display in enumerate(target_labels_for_display):\n",
    "        label_specific_indices = [idx for idx, actual_label in enumerate(all_labels) if actual_label == label_to_display]\n",
    "\n",
    "        if not label_specific_indices:\n",
    "            print(f\"No samples found for label: {class_names_map[label_to_display]}\")\n",
    "            for j in range(num_samples_per_class):\n",
    "                if i < axes.shape[0] and j < axes.shape[1]: # Check bounds\n",
    "                    ax = axes[i, j]\n",
    "                    ax.text(0.5, 0.5, 'No samples', ha='center', va='center')\n",
    "                    ax.axis('off')\n",
    "            continue\n",
    "\n",
    "        # Shuffle indices to get random samples\n",
    "        np.random.shuffle(label_specific_indices)\n",
    "\n",
    "        # Select up to num_samples_per_class\n",
    "        indices_to_show = label_specific_indices[:num_samples_per_class]\n",
    "\n",
    "        for j, sample_idx in enumerate(indices_to_show):\n",
    "            sample = dataset[sample_idx] # Fetch the actual sample using the pre-identified index\n",
    "            img = sample['image'] # PIL image\n",
    "\n",
    "            if i < axes.shape[0] and j < axes.shape[1]: # Check bounds\n",
    "                ax = axes[i, j]\n",
    "                ax.imshow(img)\n",
    "                ax.set_title(f\"{class_names_map[label_to_display]}\", fontsize=10)\n",
    "                ax.axis('off')\n",
    "            else: # Should not happen with correct subplot setup but good for safety\n",
    "                print(f\"Warning: Axes index out of bounds for plot [{i},{j}]\")\n",
    "\n",
    "    plt.suptitle(\"Sample Images from Dataset\", fontsize=14)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "\n",
    "print(\"\\nVisualizing samples from the training set:\")\n",
    "show_samples(pcam['train'], num_samples_per_class=5)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd9df8ec75ec01e8",
   "metadata": {},
   "source": [
    "## Phase 1: Baseline Model Development\n",
    "**Goal**: Develop a simple baseline model to classify tumor patches. We will create a custom PyTorch `Dataset` class to handle the PCam dataset, giving us more control over data loading and transformations.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1b27a37c20e7abe2",
   "metadata": {},
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class PatchCamelyonDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            hf_dataset (datasets.Dataset): A HuggingFace dataset split (e.g., pcam['train']).\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        image = item['image'] # This is a PIL Image\n",
    "\n",
    "        # Convert boolean label to integer (False -> 0, True -> 1)\n",
    "        label = int(item['label'])\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Ensure label is a tensor, often required by loss functions, may need to change it.\n",
    "        label = torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "        return image, label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "5539dd97f6f92ecc",
   "metadata": {},
   "source": [
    "## Task 1: **Define basic transformations**:\n",
    "1. Convert PIL Image to PyTorch Tensor\n",
    "2. Normalize the image\n",
    "   - For pre-trained models, it's common to use ImageNet statistics.\n",
    "     Mean: [0.485, 0.456, 0.406]\n",
    "     Std: [0.229, 0.224, 0.225]\n",
    "3. Basic Data Augmentation (for the training set)\n",
    "   - Random Horizontal Flip\n",
    "   - Random Vertical Flip\n",
    "   - Random Rotation (e.g., 90 degrees)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f5c57488475e842a",
   "metadata": {},
   "source": [
    "# Transformations for the training set (with augmentation)\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=90), # Rotates by a random angle up to 90 deg. Or use transforms.RandomChoice for 0,90,180,270\n",
    "    # Consider adding transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05) later\n",
    "    transforms.ToTensor(), # Converts PIL image (H, W, C) in range [0, 255] to (C, H, W) in range [0.0, 1.0]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Transformations for validation and test sets (no augmentation, just tensor conversion and normalization)\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Create instances of our custom Dataset for each split\n",
    "train_custom_dataset = PatchCamelyonDataset(pcam['train'], transform=train_transform)\n",
    "val_custom_dataset = PatchCamelyonDataset(pcam['valid'], transform=val_test_transform)\n",
    "test_custom_dataset = PatchCamelyonDataset(pcam['test'], transform=val_test_transform) # Keep test set pristine"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c7890df974099b2c",
   "metadata": {},
   "source": [
    "# Verify one sample from our custom dataset\n",
    "img_tensor, label_tensor = train_custom_dataset[0]\n",
    "print(f\"\\nSample from custom training dataset:\")\n",
    "print(f\"Image tensor shape: {img_tensor.shape}\")\n",
    "print(f\"Image tensor dtype: {img_tensor.dtype}\")\n",
    "print(f\"Label tensor: {label_tensor}\")\n",
    "print(f\"Label tensor shape: {label_tensor.shape}\")\n",
    "print(f\"Label tensor dtype: {label_tensor.dtype}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dfa70f5e2cd737af",
   "metadata": {},
   "source": [
    "# Create DataLoaders\n",
    "# DataLoaders handle batching, shuffling, and optionally parallel data loading.\n",
    "BATCH_SIZE = 64 # Adjust this based on GPU memory\n",
    "\n",
    "# num_workers = 2 was causing problems, changing it to 0 solved it {{WHY?}}\n",
    "train_dataloader = DataLoader(train_custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_custom_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_custom_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "# Fetch a batch to see its structure\n",
    "train_batch_images, train_batch_labels = next(iter(train_dataloader))\n",
    "print(f\"\\nSample batch from training DataLoader:\")\n",
    "print(f\"Images batch shape: {train_batch_images.shape}\")\n",
    "print(f\"Labels batch shape: {train_batch_labels.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b83db34289e888d7",
   "metadata": {},
   "source": [
    "# For BCEWithLogitsLoss, the label often needs to be reshaped to [BATCH_SIZE, 1] if model output is [BATCH_SIZE, 1]\n",
    "# If label_tensor was torch.tensor(label, dtype=torch.float32).unsqueeze(0) in __getitem__\n",
    "# then train_batch_labels.shape would be [BATCH_SIZE, 1]\n",
    "# Let's make that change for clarity with BCEWithLogitsLoss.\n",
    "\n",
    "print(\"\\nAdjusting label shape in Dataset for BCEWithLogitsLoss...\")\n",
    "class PatchCamelyonDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, transform=None):\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "        image = item['image']\n",
    "        label = int(item['label']) # Convert boolean to int\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # For BCEWithLogitsLoss, if model output is [batch_size, 1], target should be [batch_size, 1]\n",
    "        label = torch.tensor([label], dtype=torch.float32) # Make it [1] shape\n",
    "        return image, label"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c987b0497abcdea2",
   "metadata": {},
   "source": [
    "# Re-create datasets and dataloaders with the updated Dataset class\n",
    "train_custom_dataset = PatchCamelyonDataset(pcam['train'], transform=train_transform)\n",
    "val_custom_dataset = PatchCamelyonDataset(pcam['valid'], transform=val_test_transform)\n",
    "test_custom_dataset = PatchCamelyonDataset(pcam['test'], transform=val_test_transform)\n",
    "\n",
    "train_dataloader = DataLoader(train_custom_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, pin_memory=True)\n",
    "val_dataloader = DataLoader(val_custom_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_custom_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n",
    "\n",
    "img_tensor, label_tensor = train_custom_dataset[0]\n",
    "print(f\"\\nSample from *updated* custom training dataset:\")\n",
    "print(f\"Image tensor shape: {img_tensor.shape}\")\n",
    "print(f\"Label tensor: {label_tensor}\")\n",
    "print(f\"Label tensor shape: {label_tensor.shape}\") # Expected: torch.Size([1])\n",
    "\n",
    "train_batch_images, train_batch_labels = next(iter(train_dataloader))\n",
    "print(f\"\\nSample batch from *updated* training DataLoader:\")\n",
    "print(f\"Images batch shape: {train_batch_images.shape}\")\n",
    "print(f\"Labels batch shape: {train_batch_labels.shape}\") # Expected: torch.Size([BATCH_SIZE, 1])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "319f2ff4a436ff83",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "734edb377810930a",
   "metadata": {},
   "source": [
    "## Task 2: Model Selection & Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c367dd0a5bed03",
   "metadata": {},
   "source": [
    "Will choose a pre-trained Convolutional Neural Network (CNN) and adapt its final layer for our binary classification task (tumor vs. no tumor).\n",
    "\n",
    "Will coninue with ResNet18 as it is lightweight and effective for image classification tasks."
   ]
  },
  {
   "cell_type": "code",
   "id": "821758b6cebd66ad",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# Ensure device is set correctly\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\") # mps for Mac GPU\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- Load a pre-trained ResNet18 model ---\n",
    "# weights=models.ResNet18_Weights.IMAGENET1K_V1 ensures we get the recommended pre-trained weights\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "print(\"Original ResNet18 model structure:\")\n",
    "print(model)\n",
    "\n",
    "# --- Modify the final fully connected layer (classifier) ---\n",
    "# ResNet18's final layer is typically named 'fc'\n",
    "num_ftrs = model.fc.in_features  # Get the number of input features to the original 'fc' layer\n",
    "\n",
    "# Replace the 'fc' layer with a new nn.Linear layer.\n",
    "# For binary classification with BCEWithLogitsLoss, we need 1 output neuron.\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "\n",
    "# Move the model to the selected device\n",
    "model = model.to(device)\n",
    "\n",
    "print(\"\\nModified ResNet18 model structure (notice the 'fc' layer):\")\n",
    "print(model)\n",
    "\n",
    "# --- Test with a dummy batch of data (similar to what DataLoader would provide) ---\n",
    "# Example: Create a dummy input if dataloaders aren't handy right now\n",
    "# For ResNet18, input size is typically (N, 3, H, W), H,W >= 224 usually, but it adapts.\n",
    "# PCam patches are 96x96. Pre-trained models often work fine with smaller inputs,\n",
    "# or one might add an adaptive pooling layer, but torchvision models often handle this.\n",
    "# Our ToTensor() and Normalize() are already preparing the data.\n",
    "if 'train_batch_images' in locals() or 'train_batch_images' in globals():\n",
    "    dummy_input = train_batch_images.to(device) # Use actual batch if available\n",
    "    print(f\"\\nUsing a real batch for dummy input with shape: {dummy_input.shape}\")\n",
    "else:\n",
    "    # If train_batch_images is not defined, create a synthetic one\n",
    "    print(f\"\\nCreating a synthetic dummy input...\")\n",
    "    dummy_input = torch.randn(BATCH_SIZE, 3, 96, 96).to(device) # N, C, H, W\n",
    "\n",
    "# Perform a forward pass\n",
    "with torch.no_grad(): # No need to track gradients for this test\n",
    "    model.eval() # Set model to evaluation mode for consistent behavior (e.g., for BatchNorm)\n",
    "    output_logits = model(dummy_input)\n",
    "    model.train() # Set back to training mode for actual training later\n",
    "\n",
    "print(f\"\\nOutput logits shape: {output_logits.shape}\") # Expected: torch.Size([BATCH_SIZE, 1])\n",
    "print(f\"Example output logits (first 5): \\n{output_logits[:5]}\")\n",
    "\n",
    "# The output_logits are the raw scores before a sigmoid.\n",
    "# For BCEWithLogitsLoss, these are exactly what we need.\n",
    "# The labels from our DataLoader are shaped [BATCH_SIZE, 1] and are 0.0 or 1.0. This matches!"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1fa5036d5ef2fc59",
   "metadata": {},
   "source": [
    "## Task 3: Training Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292dc9f62b3039f3",
   "metadata": {},
   "source": [
    "### 1. Loss Function\n",
    "\n",
    "For a binary classification task where the model outputs a single raw logit, the torch.nn.BCEWithLogitsLoss is the most appropriate and numerically stable choice.\n",
    "- It combines a Sigmoid layer and the Binary Cross Entropy loss in one single class.\n",
    "- It expects raw logits as input (which our model provides) and target labels as probabilities (0.0 or 1.0 for binary, which our DataLoader provides)."
   ]
  },
  {
   "cell_type": "code",
   "id": "1472adfd14faaade",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "# Using BCEWithLogitsLoss as our model outputs raw logits (one per sample)\n",
    "# and our labels are 0.0 or 1.0.\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bda1bae541b7f905",
   "metadata": {},
   "source": [
    "### 2. Optimizer\n",
    "The optimizer is responsible for updating the model's weights based on the gradients computed during backpropagation. Adam or AdamW are popular and generally good choices as they adapt the learning rate per parameter. SGD with momentum can also work well but often requires more careful tuning of the learning rate and momentum. Let's start with Adam.\n",
    "- We need to tell the optimizer which parameters to update (i.e., all parameters of our model).\n",
    "- We also set a learning rate (lr). This is a critical hyperparameter. A common starting point is 1e-3 or 1e-4."
   ]
  },
  {
   "cell_type": "code",
   "id": "29d6e5b59a8ab85e",
   "metadata": {},
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "LEARNING_RATE = 1e-4 # A common starting point, can be tuned later\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7871518c978b99b6",
   "metadata": {},
   "source": [
    "### 3. Training and Validation Loops\n",
    "\n",
    "This is the core of the training process.\n",
    "- Training Loop (train_one_epoch function):\n",
    "    - Iterates over the train_dataloader for one epoch.\n",
    "    - Sets the model to training mode (model.train()).\n",
    "    - For each batch:\n",
    "        - Moves data and labels to the device.\n",
    "        - Clears previous gradients (optimizer.zero_grad()).\n",
    "        - Performs a forward pass to get predictions (outputs = model(inputs)).\n",
    "        - Calculates the loss (loss = criterion(outputs, labels)).\n",
    "        - Performs backpropagation to compute gradients (loss.backward()).\n",
    "        - Updates model weights (optimizer.step()).\n",
    "        - Tracks running loss and accuracy.\n",
    "\n",
    "- Validation Loop (validate_one_epoch function):\n",
    "    - Iterates over the val_dataloader.\n",
    "    - Sets the model to evaluation mode (model.eval()).\n",
    "    - Disables gradient calculations (with torch.no_grad():) as we are only evaluating, not training.\n",
    "    - Calculates loss and accuracy on the validation set. This helps monitor for overfitting.\n",
    "\n",
    "We'll also track basic metrics: loss and accuracy. Accuracy for binary classification with logits can be calculated by applying a sigmoid to the output, thresholding at 0.5, and comparing with the true labels."
   ]
  },
  {
   "cell_type": "code",
   "id": "f85d6506d1d52d56",
   "metadata": {},
   "source": [
    "from tqdm import tqdm # For progress bars\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()  # Set model to training mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    # Wrap dataloader with tqdm for a progress bar\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs) # Shape: [batch_size, 1] (logits)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # For accuracy: convert logits to probabilities, then to predictions\n",
    "        probs = torch.sigmoid(outputs) # Shape: [batch_size, 1]\n",
    "        preds = (probs > 0.5).float()  # Shape: [batch_size, 1], converts to 0.0 or 1.0\n",
    "\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions/total_samples)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc\n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Validation\", leave=False)\n",
    "\n",
    "    with torch.no_grad():  # No need to track gradients during validation\n",
    "        for inputs, labels in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "            progress_bar.set_postfix(loss=loss.item(), acc=correct_predictions/total_samples)\n",
    "\n",
    "    epoch_loss = running_loss / total_samples\n",
    "    epoch_acc = correct_predictions / total_samples\n",
    "    return epoch_loss, epoch_acc"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fb58a03a00c30381",
   "metadata": {},
   "source": [
    "### 4. Main Training Orchestration\n",
    "\n",
    "Now, let's put it all together and run the training for a few epochs."
   ]
  },
  {
   "cell_type": "code",
   "id": "cc0fb11ad021ef47",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# --- Main Training ---\n",
    "NUM_EPOCHS = 5 # Start with a small number of epochs for this baseline\n",
    "\n",
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {epoch+1} Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_dataloader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1} Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % 5 == 0: # Save every 5 epochs, or based on best validation accuracy\n",
    "        torch.save(model.state_dict(), f\"pcam_resnet18_epoch_{epoch+1}.pth\")\n",
    "\n",
    "print(\"\\nFinished Training.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8c6867854cc6e68e",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- Training Progress:\n",
    "    - **Training Loss** decreased consistently from `0.2155` to `0.1087`. This shows that the model is learning from the training data.\n",
    "    - **Training Accuracy** increased steadily from `0.9144` (91.44%) to `0.9604` (96.04%). This also indicates effective learning on the training set.\n",
    "- Validation Performance:\n",
    "    - **Validation Loss** started at `0.2894`, increased in epoch to `0.3731`, and then fluctuated around `0.28 - 0.33`. It didn't consistently decrease like the training loss. By Epoch 5, it's `0.3384`, which is higher than the training loss.\n",
    "    - **Validation Accuracy** started at `0.8860` (88.60%), dipped slightly, then hovered around `0.88 - 0.89` (88 - 89%), ending at `0.8875` in Epoch 5. It is noticeably lower than the final training accuracy.\n",
    "- Key Observation:\n",
    "    - Clear gap between training performance and validation performance.\n",
    "    - This is a sign of **OVERFITTING**. The model learned the data very well, perhaps even memorizing some of its specific noise and patterns, but it's not generalizing as effectively to the unseen validation data. The validation loss even started to trend upwards or stagnate while training loss continued to decrease, which is a strong indicator."
   ]
  },
  {
   "cell_type": "code",
   "id": "478c1482dce97c9b",
   "metadata": {},
   "source": [
    "# --- Use saved trained model ---\n",
    "# Define the model architecture (should be same as during training)\n",
    "eval_model = models.resnet18(weights=None) # Start with an uninitialized model\n",
    "num_ftrs = eval_model.fc.in_features\n",
    "eval_model.fc = nn.Linear(num_ftrs, 1) # Adapt for binary classification\n",
    "\n",
    "# Define the path to your saved model\n",
    "MODEL_PATH = \"pcam_resnet18_epoch_5.pth\"\n",
    "\n",
    "# Load the state dictionary\n",
    "try:\n",
    "    eval_model.load_state_dict(torch.load(MODEL_PATH, map_location=device))\n",
    "    print(f\"Successfully loaded model weights from {MODEL_PATH}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Model file not found at {MODEL_PATH}. Please check the path.\")\n",
    "    print(\"Ensure you have trained and saved a model checkpoint.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the model: {e}\")\n",
    "\n",
    "eval_model = eval_model.to(device)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "17ee49d7c75996c5",
   "metadata": {},
   "source": [
    "## Task 4: Detailed Evaluation on Test Set\n",
    "**Goal:** Assess the generalization performance of the trained ResNet18 model on unseen data. This is important to understand how well the model might perform on a real-world scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b1735ce7ab0e16a6",
   "metadata": {},
   "source": [
    "# --- Prepare for Evaluation & Collect Predictions and True Labels ---\n",
    "eval_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_probabilities = [] # For ROC AUC\n",
    "\n",
    "with torch.no_grad(): # Disable gradient calculations (no backpropagation, no need)\n",
    "    progress_bar = tqdm(test_dataloader, desc=\"Evaluating on Test Set\", leave=False)\n",
    "    for inputs, labels in progress_bar:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device) # Shape: [batch_size, 1]\n",
    "\n",
    "        outputs = eval_model(inputs) # Logits, Shape: [batch_size, 1]\n",
    "\n",
    "        # Probabilities for ROC AUC\n",
    "        probs = torch.sigmoid(outputs) # Shape: [batch_size, 1]\n",
    "        all_probabilities.extend(probs.cpu().numpy()) # Store probabilities\n",
    "\n",
    "        # Predictions (0 or 1)\n",
    "        preds = (probs > 0.5).float() # Shape: [batch_size, 1]\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Convert lists of arrays/tensors to single NumPy arrays\n",
    "# Ensure they are 1D for scikit-learn metrics\n",
    "all_labels = np.array(all_labels).flatten()\n",
    "all_predictions = np.array(all_predictions).flatten()\n",
    "all_probabilities = np.array(all_probabilities).flatten() # Probabilities for the positive class\n",
    "\n",
    "print(\"\\nEvaluation complete. Collected predictions and labels.\")\n",
    "print(f\"Shape of all_labels: {all_labels.shape}\")\n",
    "print(f\"Shape of all_predictions: {all_predictions.shape}\")\n",
    "print(f\"Shape of all_probabilities: {all_probabilities.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4a6046c0e502635a",
   "metadata": {},
   "source": [
    "# --- Calculate Performance Metrics ---\n",
    "accuracy = accuracy_score(all_labels, all_predictions)\n",
    "# For precision, recall, f1_score:\n",
    "#   - 'binary' assumes positive label is 1 (which is our case: 1 for tumor)\n",
    "#   - zero_division=0 handles cases where a class might not be predicted, avoiding warnings.\n",
    "precision = precision_score(all_labels, all_predictions, pos_label=1, zero_division=0)\n",
    "recall = recall_score(all_labels, all_predictions, pos_label=1, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_predictions, pos_label=1, zero_division=0)\n",
    "roc_auc = roc_auc_score(all_labels, all_probabilities) # Use probabilities for ROC AUC\n",
    "\n",
    "print(\"\\n--- Test Set Performance Metrics ---\")\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f} (for class 1 - Tumor)\")\n",
    "print(f\"Recall:    {recall:.4f} (for class 1 - Tumor)\")\n",
    "print(f\"F1-score:  {f1:.4f} (for class 1 - Tumor)\")\n",
    "print(f\"AUC-ROC:   {roc_auc:.4f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e9d0b7cef24e9f6e",
   "metadata": {},
   "source": [
    "**Interpretation:**\n",
    "- Accuracy: `0.8411` (84.11%)\n",
    "    - Overall, 84.11% of the test patches were classified correctly (either as tumor or no-tumor).\n",
    "    - This is a decent starting point, but it is lower than both the final training accuracy (96.0%) and the final validation accuracy (88.8%). This reinforces the overfitting observation (performance drops when moving from seen data to truly unseen test data).\n",
    "- Precision (for class 1 – Tumor): `0.9623` (96.23%)\n",
    "    - When the model predicts a patch _is_ a tumor, it is correct 96.23% of the time.\n",
    "    - This is a very high value! It means that the model produces very few False Positives (FP). If it says \"tumor\" you cna be quite confident it is indeed a tumor.\n",
    "- Recall (for class 1 – Tumor): `0.7100` (71.00%)\n",
    "    - Out of all the patches that _actually are_ tumors, the model correctly identified 71.00% of them.\n",
    "    - THis is significantly lower than the precision. It means the model misses a considerable number of actual tumors, leading to many False Negatives (FN). This is a main weakness.\n",
    "- F1-score (for class 1 – Tumor): `0.8171` (81.71%)\n",
    "    - The harmonic mean of precision and recall. It provides a single score that balances both.\n",
    "    - It's pulled down from the high precision due to the lower recall. An F1-score of ~0.82 suggests reasonable performance, but stresses that the imbalance between precision and recall (specifically, the low recall) is an issue.\n",
    "- AUC-ROC: `0.9493` (~0.95)\n",
    "    - The Area Under the Receiver Operating Characteristic Curve. It measures the model's ability to distinguish between the positive (tumor) and negative (no-tumor) classes across _all possible classification thresholds_. an AUC of 1.0 is perfect, and 0.5 is random guessing.\n",
    "    - The ~0.95 score is a very good AUC score. It suggests that the model has learned features that are inherently quite discriminative between the two classes. Even if the current default threshold of 0.5 (used for precision/recall/accuracy) isn't optimal for recall, the underlying ability to separate classes is strong. This is a positive sign for potential improvement."
   ]
  },
  {
   "cell_type": "code",
   "id": "888f689cbb5f85ec",
   "metadata": {},
   "source": [
    "# --- Visualize Performance (Enhanced Confusion Matrix) ---\n",
    "\n",
    "# Calculate Confusion Matrix\n",
    "cm = confusion_matrix(all_labels, all_predictions)\n",
    "tn, fp, fn, tp = cm.ravel() # Extracts values in order: TN, FP, FN, TP\n",
    "\n",
    "# Calculate percentages\n",
    "total_samples_in_cm = np.sum(cm)\n",
    "cm_percent = cm / total_samples_in_cm # Cell percentages\n",
    "\n",
    "# Create labels for each cell: \"Label\\nValue\\n(Percentage%)\"\n",
    "group_names = ['True Neg (TN)', 'False Pos (FP)', 'False Neg (FN)', 'True Pos (TP)']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten()/np.sum(cm)]\n",
    "\n",
    "# Combine labels for heatmap annotation\n",
    "annot_labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in zip(group_names, group_counts, group_percentages)]\n",
    "annot_labels = np.asarray(annot_labels).reshape(2,2) # Reshape to 2x2 for the heatmap\n",
    "\n",
    "# Class labels for axes\n",
    "class_labels = ['No Tumor (0)', 'Tumor (1)']\n",
    "\n",
    "plt.figure(figsize=(8, 7))\n",
    "sns.heatmap(cm, annot=annot_labels, fmt='', cmap='Blues',\n",
    "            xticklabels=class_labels, yticklabels=class_labels,\n",
    "            annot_kws={\"size\": 12, \"va\": \"center_baseline\", \"ha\": \"center\"}) # Adjust font size and alignment\n",
    "\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.title('Confusion Matrix - Test Set', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "# The ROC Curve plotting remains the same:\n",
    "fpr, tpr, thresholds = roc_curve(all_labels, all_probabilities, pos_label=1)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR) / Recall')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve - Test Set')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1f2c943755e61bd8",
   "metadata": {},
   "source": [
    "## Task 5: Results Logging and Visualization (of Training Process)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c46a17e41b5e15",
   "metadata": {},
   "source": [
    "Let's start by adjusting the main training loop to collect the training loss, training accuracy, validation loss, validation accuracy values for each of the epoch into lists. Storing these metrics will be useful for visualization."
   ]
  },
  {
   "cell_type": "code",
   "id": "a27be5ba5565ed3d",
   "metadata": {
    "tags": [
     "skip"
    ]
   },
   "source": [
    "# # --- Ensure these are re-initialized for a fresh run ---\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 1)\n",
    "model = model.to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "NUM_EPOCHS = 15 # Or more, if you want to re-run for a longer period\n",
    "LEARNING_RATE = 1e-4\n",
    "\n",
    "history = {\n",
    "    'train_loss': [],\n",
    "    'train_acc': [],\n",
    "    'val_loss': [],\n",
    "    'val_acc': []\n",
    "}\n",
    "\n",
    "best_val_accuracy = 0.0  # Initialize best validation accuracy\n",
    "best_epoch = 0           # To store the epoch number of the best model\n",
    "PATH_BEST_MODEL = \"pcam_resnet18_best_model.pth\" # Path to save the best model\n",
    "\n",
    "print(f\"\\nStarting training for {NUM_EPOCHS} epochs (with metric logging and best model saving)...\")\n",
    "print(f\"Best model will be saved to: {PATH_BEST_MODEL}\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    current_epoch_num = epoch + 1\n",
    "    print(f\"\\n--- Epoch {current_epoch_num}/{NUM_EPOCHS} ---\")\n",
    "\n",
    "    # Train one epoch\n",
    "    train_loss, train_acc = train_one_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    print(f\"Epoch {current_epoch_num} Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.4f}\")\n",
    "\n",
    "    # Validate one epoch\n",
    "    val_loss, val_acc = validate_one_epoch(model, val_dataloader, criterion, device)\n",
    "    print(f\"Epoch {current_epoch_num} Validation: Loss = {val_loss:.4f}, Accuracy = {val_acc:.4f}\")\n",
    "\n",
    "    # Store metrics\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    # Check if this is the best model so far based on validation accuracy\n",
    "    if val_acc > best_val_accuracy:\n",
    "        best_val_accuracy = val_acc\n",
    "        best_epoch = current_epoch_num\n",
    "        # Save the model's state dictionary\n",
    "        torch.save(model.state_dict(), PATH_BEST_MODEL)\n",
    "        print(f\"🎉 New best model saved! Validation Accuracy: {best_val_accuracy:.4f} at Epoch {best_epoch}\")\n",
    "\n",
    "    # Optional: Periodic saving if you want multiple checkpoints\n",
    "    if current_epoch_num % 5 == 0:\n",
    "        periodic_save_path = f\"pcam_resnet18_epoch_{current_epoch_num}_checkpoint.pth\"\n",
    "        torch.save(model.state_dict(), periodic_save_path)\n",
    "        print(f\"Saved periodic checkpoint to {periodic_save_path}\")\n",
    "\n",
    "print(\"\\nFinished Training.\")\n",
    "print(f\"Best validation accuracy achieved: {best_val_accuracy:.4f} at Epoch {best_epoch}\")\n",
    "print(f\"Best model parameters saved to: {PATH_BEST_MODEL}\")\n",
    "\n",
    "# --- At this point, 'history' dictionary contains all the metrics ---\n",
    "print(\"\\nCollected metrics history:\")\n",
    "print(history)\n",
    "\n",
    "# Later, loading the best model later for evaluation:\n",
    "best_model_state_dict = torch.load(PATH_BEST_MODEL)\n",
    "eval_model.load_state_dict(best_model_state_dict)\n",
    "eval_model.to(device)\n",
    "eval_model.eval()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9bb4953127749880",
   "metadata": {},
   "source": [
    "# # Manual values from previous run, avoiding other 6.5 hrs of training\n",
    "# history = {\n",
    "#     'train_loss': [0.2155, 0.1537, 0.1325, 0.1181, 0.1087],\n",
    "#     'train_acc':  [0.9144, 0.9419, 0.9511, 0.9569, 0.9604],\n",
    "#     'val_loss':   [0.2894, 0.3731, 0.2856, 0.3278, 0.3384],\n",
    "#     'val_acc':    [0.8860, 0.8669, 0.8919, 0.8869, 0.8875]\n",
    "# }"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a891a4f088482548",
   "metadata": {},
   "source": [
    "def plot_learning_curves(history, title_suffix=\"\"):\n",
    "    epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(14, 5))\n",
    "\n",
    "    # Plot Training & Validation Loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs_range, history['train_loss'], label='Training Loss')\n",
    "    plt.plot(epochs_range, history['val_loss'], label='Validation Loss')\n",
    "    plt.title(title_suffix + 'Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot Training & Validation Accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, history['train_acc'], label='Training Accuracy')\n",
    "    plt.plot(epochs_range, history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.suptitle('Learning Curves', fontsize=16)\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96]) # Adjust layout to make space for suptitle\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "15a80d6aca501c9c",
   "metadata": {},
   "source": [
    "plot_learning_curves(history)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e0c978211448fd86",
   "metadata": {},
   "source": [
    "**Key Takeaways from these Curves**:\n",
    "- **Clear Overfitting**: The most obvious takeaway is the difference between the training and validation curves.\n",
    "    - Training loss goes down, validation loss goes up (or fluctuates much higher).\n",
    "    - Training accuracy goes up, validation accuracy plateaus well below it (or even starts to decrease).\n",
    "    - This means the model is becoming too specialized to the training data and losing its ability to generalize to new, unseen data.\n",
    "\n",
    "- **Optimal Point for Early Stopping** (Based on these 5 Epochs):\n",
    "    - If we were using validation loss as the factor for early stopping, Epoch 3 looks like the best point. It has the lowest validation loss (0.2856). After this point, the validation loss starts to increase.\n",
    "    - If we were using validation accuracy, Epoch 3 also gives the highest validation accuracy (0.8919).\n",
    "    - So, based only on these 5 epochs, a model saved at the end of Epoch 3 might have performed slightly better on the test set than the model saved at Epoch 5.\n",
    "\n",
    "- **The Spike in Validation Loss at Epoch 2**: This is interesting. It suggests that at that point, the model learned something from the training data that was actively detrimental to its performance on the validation set. It then \"corrected\" somewhat in Epoch 3 before overfitting signs became more dominant again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182d7d2f5cf27236",
   "metadata": {},
   "source": [
    "# Phase 2: Experimentation and Model Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7d427c0e2fcc77",
   "metadata": {},
   "source": [
    "## Task 1: Hyperparameter Tuning (Simple Introduction)\n",
    "**Goal**: Understand the impact of key hyperparameters on model training and performance.\n",
    "\n",
    "Will start with the Learning Rate. We have been using `LEARNING_RATE = 1e-4` as our starting point. Let's experiment to see it's effect."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0c402aec95b1ae0",
   "metadata": {},
   "source": [
    "def run_lr_experiment(learning_rate, num_epochs, model_architecture,\n",
    "                      train_loader, val_loader, criterion_class, optimizer_class, device,\n",
    "                      experiment_name=\"\"):\n",
    "    \"\"\"\n",
    "    Runs a training experiment for a given learning rate.\n",
    "\n",
    "    Args:\n",
    "        learning_rate (float): The learning rate to use.\n",
    "        num_epochs (int): Number of epochs to train.\n",
    "        model_architecture (callable): A function that returns an uninitialized model (e.g., models.resnet18).\n",
    "        train_loader (DataLoader): DataLoader for training data.\n",
    "        val_loader (DataLoader): DataLoader for validation data.\n",
    "        criterion_class (callable): The loss function class (e.g., nn.BCEWithLogitsLoss).\n",
    "        optimizer_class (callable): The optimizer class (e.g., optim.Adam).\n",
    "        device (torch.device): The device to train on.\n",
    "        experiment_name (str): A name for this experiment (e.g., \"lr_1e-3\") for saving files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (history, best_validation_accuracy, best_epoch_number)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Running Experiment: {experiment_name} (LR: {learning_rate}) ---\")\n",
    "\n",
    "    # Re-initialize Model\n",
    "    model = model_architecture(weights=models.ResNet18_Weights.IMAGENET1K_V1) # Assuming ResNet18 structure\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, 1)\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Re-initialize Optimizer\n",
    "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Re-initialize Criterion\n",
    "    criterion = criterion_class()\n",
    "\n",
    "    # Training Loop\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': []\n",
    "    }\n",
    "    best_val_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    path_best_model = f\"pcam_resnet18_best_model_{experiment_name}.pth\"\n",
    "\n",
    "    print(f\"Starting training for {num_epochs} epochs...\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        current_epoch_num = epoch + 1\n",
    "\n",
    "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device) # globally defined or imported\n",
    "        val_loss, val_acc = validate_one_epoch(model, val_loader, criterion, device) # globally defined or imported\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {current_epoch_num}/{num_epochs} | Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
    "\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = current_epoch_num\n",
    "            torch.save(model.state_dict(), path_best_model)\n",
    "            print(f\"  🎉 New best model saved! Val Acc: {best_val_acc:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinished Training for LR={learning_rate}.\")\n",
    "    print(f\"Best val acc for LR {learning_rate}: {best_val_acc:.4f} at Epoch {best_epoch}\")\n",
    "    print(f\"Best model saved to: {path_best_model}\")\n",
    "\n",
    "    plot_learning_curves(history, title_suffix=f\" (LR={learning_rate}) \")\n",
    "\n",
    "    return history, best_val_acc, best_epoch\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "79db7030698f53bc",
   "metadata": {},
   "source": [
    "NUM_EPOCHS_FOR_LR_TUNING = 10 # relatively short for now, quick comparison\n",
    "lrs_to_test = [1e-3, 1e-5] # Compare with original 1e-4\n",
    "\n",
    "# Store results from all LR experiments\n",
    "lr_experiment_results = {}\n",
    "\n",
    "for lr in lrs_to_test:\n",
    "    exp_name = f\"lr_{lr:.0e}\".replace('-', '_minus_') # Creates a file-friendly name like \"lr_1e_minus_03\"\n",
    "    hist, b_val_acc, b_epoch = run_lr_experiment(\n",
    "        learning_rate=lr,\n",
    "        num_epochs=NUM_EPOCHS_FOR_LR_TUNING,\n",
    "        model_architecture=models.resnet18, # Pass the function itself\n",
    "        train_loader=train_dataloader,\n",
    "        val_loader=val_dataloader,\n",
    "        criterion_class=nn.BCEWithLogitsLoss, # Pass the class\n",
    "        optimizer_class=optim.Adam,         # Pass the class\n",
    "        device=device,\n",
    "        experiment_name=exp_name\n",
    "    )\n",
    "    lr_experiment_results[lr] = {'history': hist, 'best_val_acc': b_val_acc, 'best_epoch': b_epoch}\n",
    "\n",
    "print(\"\\n--- All LR Experiments Summary ---\")\n",
    "for lr, result in lr_experiment_results.items():\n",
    "    print(f\"LR: {lr:.0e} -> Best Val Acc: {result['best_val_acc']:.4f} at Epoch {result['best_epoch']}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1b1a8e4900e8cde6",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
